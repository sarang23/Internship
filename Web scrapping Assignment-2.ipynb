{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------Web Scrapping  Assignement -2------------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Selenium \n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all required libraries \n",
    "import selenium\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# Opening Browsers in Incognito using Options\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--incognito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data from Naukri.com\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding Element for Job Search.\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Banglore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on Search Button\n",
    "search_btn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the url of webpage to be scraped\n",
    "url=\"https://www.naukri.com/data-analyst-jobs-in-banglore?k=data%20analyst&l=banglore\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create 3 empty list for our requirment as Job title, Location and Company name \n",
    "\n",
    "job_titles=[]\n",
    "company_name=[]\n",
    "location_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can extract all the title tags haiving job titles and we have to find first 10 job titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles_tags:\n",
    "    title=i.text\n",
    "    job_titles.append(title)\n",
    "job_titles[0:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can extract all the title tags haiving job titles and we have to find first 10 companies Name\n",
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in companies_tags:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "company_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can extract all the title tags haiving job titles and we have to find first 10 Location List\n",
    "location_tags=driver.find_elements_by_xpath(\".//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "location_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in location_tags:\n",
    "    location=i.text\n",
    "    location_list.append(location)\n",
    "location_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the length of lists\n",
    "print(len(location_list),len(company_name),len(job_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now we will make a data frame\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Title']=job_titles\n",
    "jobs['Company Name']=company_name\n",
    "jobs['Location Name']=location_list\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function opens the naukri home page and searches for 'Data Scientist' and 'Bangalore'\n",
    "\n",
    "def naukri_desc(url):\n",
    "    driver=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "\n",
    "    # 2. Enter “Data Scientist” in “Skill,Designations,Companies” field\n",
    "\n",
    "    from selenium.common.exceptions import NoSuchElementException\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        job_search=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "        job_search.send_keys(\"Data Scientist\")\n",
    "    except NoSuchElementException as e:\n",
    "        print(\"Exception Raised : \", e)\n",
    "\n",
    "    # and enter “Bangalore” in “enter the location” field \n",
    "    location_search=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "    location_search.send_keys(\"Bangalore\")\n",
    "\n",
    "    # 3. Then click the search button\n",
    "    search_buton=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    search_buton.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    jobs=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[:10]\n",
    "\n",
    "    job_titles=[]\n",
    "    company_names=[]\n",
    "    desc=[]\n",
    "\n",
    "    from selenium.common.exceptions import NoSuchElementException         # Importing Exception\n",
    "\n",
    "    for b in jobs:\n",
    "        url=b.get_attribute('href')\n",
    "        driver_1=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "        driver_1.get(url)\n",
    "        try:\n",
    "            i=driver_1.find_element_by_xpath(\"//h1[@class='jd-header-title']\")\n",
    "            j=driver_1.find_element_by_xpath(\"//a[@class='pad-rt-8']\")\n",
    "            k=driver_1.find_elements_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "        except NoSuchElementException as e:\n",
    "            i=driver_1.find_element_by_xpath(\"//h1[@class='av-special-heading-tag ']\")\n",
    "            j=driver_1.find_element_by_xpath(\"//p[@class='cpName f14']\")\n",
    "            k=driver_1.find_elements_by_xpath(\"//div[@class='clearboth description']\")\n",
    "\n",
    "\n",
    "        job_titles.append(i.text)\n",
    "        company_names.append(j.text)        \n",
    "        # Collecting full description\n",
    "        desc_content=[]\n",
    "        for a in k:\n",
    "            desc_content.append(j.text.replace('\\n',' '))\n",
    "        desc.append(desc_content)\n",
    "\n",
    "        driver_1.close()\n",
    "    \n",
    "    jobs_2=pd.DataFrame({})\n",
    "    jobs_2['job_titles']=job_titles\n",
    "    jobs_2['company_names']=company_names\n",
    "    jobs_2['Full Job Description']=desc\n",
    "    \n",
    "    driver.close()\n",
    "    return(jobs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=naukri_desc('https://www.naukri.com/')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function opens the naukri home page and searches for 'Data Analyst' and 'Bangalore'\n",
    "\n",
    "def naukri_advance_filters(url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--incognito')\n",
    "    driver=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "    # 1. first get the webpage https://www.naukri.com/\n",
    "    driver.get(url)\n",
    "    \n",
    "    # 2. Enter “Data Scientist” in “Skill,Designations,Companies” field \n",
    "    job_search=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "    job_search.send_keys(\"Data Scientist\")\n",
    " \n",
    "    # 3. Then click the search button\n",
    "    search_buton=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    search_buton.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "\n",
    "    chk=driver.find_element_by_xpath(\"//div[@class='mt-8 chckBoxCont']/label[@for='chk-Delhi / NCR-cityTypeGid-']/i\")\n",
    "    chk.click()\n",
    "    time.sleep(2)\n",
    "    chk=driver.find_element_by_xpath(\"//div[@class='mt-8 chckBoxCont']/label[@for='chk-3-6 Lakhs-ctcFilter-']/i\")\n",
    "    chk.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 5. Then scrape the data for the first 10 jobs results you get.\n",
    "    job_titles=[]\n",
    "    title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[:10]\n",
    "    for i in title_tags:\n",
    "        job_titles.append(i.text)\n",
    "    \n",
    "    company_names=[]\n",
    "    company_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[:10]\n",
    "    for i in company_tags:\n",
    "        company_names.append(i.text)\n",
    "        \n",
    "    experience_list=[]\n",
    "    exp_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")[:10]\n",
    "    for i in exp_tags:\n",
    "        experience_list.append(i.text)\n",
    "        \n",
    "    locations_list=[]\n",
    "    loc_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")[:10]\n",
    "    for i in loc_tags:\n",
    "        locations_list.append(i.text)\n",
    "        \n",
    "    jobs=pd.DataFrame({})\n",
    "    jobs['Job Title']=job_titles\n",
    "    jobs['Company Name']=company_names\n",
    "    jobs['Location']=locations_list\n",
    "    jobs['Experience']=experience_list\n",
    "    \n",
    "    driver.close()\n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=naukri_advance_filters('https://www.naukri.com/')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def glassdoor(url):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "\n",
    "    driver = webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Sign in Option\n",
    "    sign_in=driver.find_element_by_xpath(\"//div[@class='locked-home-sign-in']/a\")\n",
    "    url=sign_in.get_attribute('href')\n",
    "    driver_2=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "    driver_2.get(url)\n",
    "    time.sleep(2)\n",
    "    email=driver_2.find_element_by_id('userEmail')\n",
    "    email.send_keys('sarangkatre@gmail.com')\n",
    "    passw=driver_2.find_element_by_id('userPassword')\n",
    "    passw.send_keys('Sarang@123')\n",
    "    sign_in_button=driver_2.find_element_by_xpath(\"//div/button[@class='gd-ui-button minWidthBtn css-8i7bc2']\")\n",
    "    sign_in_button.click()\n",
    "    time.sleep(2)\n",
    "    driver.close()\n",
    "    \n",
    "    # searching required fields\n",
    "    job_search=driver_2.find_element_by_id('sc.keyword')\n",
    "    job_search.send_keys('Data Scientist')\n",
    "    \n",
    "    location=driver_2.find_element_by_id('sc.location')\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    location.send_keys(Keys.CONTROL + \"a\")\n",
    "    location.send_keys(Keys.DELETE)\n",
    "    location.send_keys('Noida')\n",
    "    \n",
    "    search_button=driver_2.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto SearchStyles__newSearchButton css-iixdfr']\")\n",
    "    search_button.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    company_name=[]\n",
    "    days_posted=[]\n",
    "    rating=[]\n",
    "    \n",
    "    # Company and days posted are available but for ratings need to open in new window.\n",
    "    # Opening new window\n",
    "    \n",
    "    from selenium.common.exceptions import NoSuchElementException         # Importing Exception\n",
    "    new_page=driver_2.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']/a\")[:10]\n",
    "    \n",
    "    for i in new_page:\n",
    "        url=i.get_attribute('href')\n",
    "        driver_3=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "        driver_3.get(url)\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            a=driver_3.find_element_by_xpath(\"//span[@class='css-1pmc6te e11nt52q4']\")\n",
    "            rating.append(a.text.replace('\\n★',''))\n",
    "        except NoSuchElementException as e:\n",
    "            rating.append(\"No Rating\")\n",
    "        driver_3.close() \n",
    "       \n",
    "    comps=driver_2.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']/a/span\")[:10]\n",
    "    for i in comps:\n",
    "        company_name.append(i.text)\n",
    "        \n",
    "    days=driver_2.find_elements_by_xpath(\"//div[@data-test='job-age']\")[:10]\n",
    "    for i in days:\n",
    "        days_posted.append(i.text)\n",
    "         \n",
    "    jobs=pd.DataFrame({})\n",
    "    jobs['Company Name']=company_name\n",
    "    jobs['Days Since Posted']=days_posted\n",
    "    jobs['Rating']=rating\n",
    "    \n",
    "    driver_2.close()\n",
    "    \n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=glassdoor('https://www.glassdoor.co.in/index.htm')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 5 Write a python program to scrape the salary data for Data Scientist designation in Noida location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glassdoor_2(url):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "\n",
    "    driver = webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "     \n",
    "    # searching required fields\n",
    "    job_search=driver.find_element_by_id('KeywordSearch')\n",
    "    job_search.send_keys('Data Scientist')\n",
    "    \n",
    "    location=driver.find_element_by_id('LocationSearch')\n",
    "    location.clear()\n",
    "    location.send_keys('Noida')\n",
    "        \n",
    "    search_buton=driver.find_element_by_id(\"HeroSearchButton\")\n",
    "    search_buton.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    comp=[]\n",
    "    number_of_salaries=[]\n",
    "    avg_salary=[]\n",
    "    min_salary=[]\n",
    "    max_salary=[]\n",
    "    \n",
    "    a=driver.find_elements_by_xpath(\"//div[@data-test='job-info']/p[2]\")[:10]\n",
    "    for i in a:\n",
    "        comp.append(i.text)\n",
    "        \n",
    "    b=driver.find_elements_by_xpath(\"//p[@class='css-1uyte9r css-1kuy7z7 m-0 ']\")[:10]\n",
    "    for i in b:\n",
    "        number_of_salaries.append(i.text.replace(' salaries',''))\n",
    "        \n",
    "    c=driver.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']/strong\")[:10]\n",
    "    for i in c:\n",
    "        avg_salary.append(i.text)\n",
    "        \n",
    "    d=driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values d-flex justify-content-between ']/span[1]\")[:10]\n",
    "    for i in d:\n",
    "        min_salary.append(i.text)\n",
    "        \n",
    "    e=driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values d-flex justify-content-between ']/span[2]\")[:10]\n",
    "    for i in e:\n",
    "        max_salary.append(i.text)\n",
    "         \n",
    "    jobs=pd.DataFrame({})\n",
    "    jobs['company_names']=comp\n",
    "    jobs['number_of_salaries']=number_of_salaries\n",
    "    jobs['avg_salary']=avg_salary\n",
    "    jobs['min_salary']=min_salary\n",
    "    jobs['max_salary']=max_salary\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=glassdoor_2('https://www.glassdoor.co.in/Salaries/index.htm')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipkart_100(url):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "\n",
    "    driver = webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    #Closing Pop-up\n",
    "    close_button=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "    close_button.click()    \n",
    "    \n",
    "    \n",
    "    # searching required fields\n",
    "    search=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']/input\")\n",
    "    search.send_keys('sunglasses')\n",
    "    \n",
    "    search_button=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "    search_button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    brand=[]\n",
    "    des=[]\n",
    "    price=[]\n",
    "    discount=[]\n",
    "    \n",
    "    def extract(driver,number):\n",
    "                \n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")[:number]\n",
    "        for i in a:\n",
    "            brand.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")[:number]\n",
    "        for i in a:\n",
    "            des.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[2]/div/div[1]\")[:number]\n",
    "        for i in a:\n",
    "            price.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[2]/div/div[3]\")[:number]\n",
    "        for i in a:\n",
    "            discount.append(i.text)\n",
    "    \n",
    "    while(len(discount)<40):\n",
    "        extract(driver,40)\n",
    "        \n",
    "    while(len(discount)<80):\n",
    "        next=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "        driver_2=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "        url=next.get_attribute('href')\n",
    "        driver_2.get(url)\n",
    "        extract(driver_2,40)\n",
    "    \n",
    "    while(len(discount)<100):\n",
    "        next=driver_2.find_element_by_xpath(\"//a[@class='_1LKTO3'][2]\")\n",
    "        driver_3=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "        url=next.get_attribute('href')\n",
    "        driver_3.get(url)\n",
    "        extract(driver_3,20)  \n",
    "    \n",
    "    jobs=pd.DataFrame({})\n",
    "    jobs['Brand']=brand\n",
    "    jobs['Description']=des\n",
    "    jobs['Price']=price\n",
    "    jobs['Discount']=discount\n",
    "    \n",
    "    driver.close()\n",
    "    driver_2.close()\n",
    "    driver_3.close()\n",
    "    \n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=flipkart_100('https://www.flipkart.com/')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipkart_reviews(url):\n",
    "\n",
    "    driver_1 = webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "    driver_1.get(url)\n",
    "    \n",
    "    # Opening full reviews\n",
    "    full=driver_1.find_element_by_xpath(\"//div[@class='col JOpGWq']/a\")\n",
    "    driver=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "    url=full.get_attribute('href')\n",
    "    driver.get(url)\n",
    "    driver_1.close()\n",
    "       \n",
    "    rating=[]\n",
    "    review_summary=[]\n",
    "    full_review=[]\n",
    "    \n",
    "    def extract(driver):\n",
    "                \n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "        for i in a:\n",
    "            rating.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "        for i in a:\n",
    "            review_summary.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "        for i in a:\n",
    "            full_review.append(i.text)\n",
    "    \n",
    "    while(len(full_review)<10):\n",
    "        extract(driver)\n",
    "        \n",
    "    while(len(full_review)<100):\n",
    "        next=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "        driver=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "        url=next.get_attribute('href')\n",
    "        driver.get(url)\n",
    "        extract(driver)  \n",
    "    \n",
    "    jobs=pd.DataFrame({})\n",
    "    jobs['Rating']=rating\n",
    "    jobs['Review Summary']=review_summary\n",
    "    jobs['Full Review']=full_review\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=flipkart_reviews('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipkart_100_sneakers(url):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "\n",
    "    driver = webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    #Closing Pop-up\n",
    "    close_button=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "    close_button.click()    \n",
    "    \n",
    "    \n",
    "    # searching required fields\n",
    "    search=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']/input\")\n",
    "    search.send_keys('sneakers')\n",
    "    \n",
    "    search_button=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "    search_button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    brand=[]\n",
    "    des=[]\n",
    "    price=[]\n",
    "    discount=[]\n",
    "    \n",
    "    def extract(driver,number):\n",
    "                \n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")[:number]\n",
    "        for i in a:\n",
    "            brand.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")[:number]\n",
    "        for i in a:\n",
    "            des.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[2]/div/div[1]\")[:number]\n",
    "        for i in a:\n",
    "            price.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[2]/div/div[3]\")[:number]\n",
    "        for i in a:\n",
    "            discount.append(i.text)\n",
    "    \n",
    "    while(len(discount)<40):\n",
    "        extract(driver,40)\n",
    "        \n",
    "    while(len(discount)<80):\n",
    "        next=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "        driver_2=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "        url=next.get_attribute('href')\n",
    "        driver_2.get(url)\n",
    "        extract(driver_2,40)\n",
    "    \n",
    "    while(len(discount)<100):\n",
    "        next=driver_2.find_element_by_xpath(\"//a[@class='_1LKTO3'][2]\")\n",
    "        driver_3=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "        url=next.get_attribute('href')\n",
    "        driver_3.get(url)\n",
    "        extract(driver_3,20)  \n",
    "    \n",
    "    jobs=pd.DataFrame({})\n",
    "    jobs['Brand']=brand\n",
    "    jobs['Description']=des\n",
    "    jobs['Price']=price\n",
    "    jobs['Discount']=discount\n",
    "    \n",
    "    driver.close()\n",
    "    driver_2.close()\n",
    "    driver_3.close()\n",
    "    \n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=flipkart_100_sneakers('https://www.flipkart.com/')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myntra(url):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "\n",
    "    driver = webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "\n",
    "    # Adding required filters\n",
    "    # [Actual Value for this search = Rs. 7649 to Rs. 15099]\n",
    "    price=driver.find_element_by_xpath(\"//ul[@class='price-list']/li[2]/label/div\")\n",
    "    price.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    color=driver.find_element_by_xpath(\"//li[@class='colour-listItem']/label/div\")\n",
    "    color.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    brand=[]\n",
    "    des=[]\n",
    "    price=[]\n",
    "    \n",
    "    def extract_2(driver,number):\n",
    "                \n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h3\")[:number]\n",
    "        for i in a:\n",
    "            brand.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h4[1]\")[:number]\n",
    "        for i in a:\n",
    "            des.append(i.text)\n",
    "\n",
    "        a=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/div/span[1]\")[:number]\n",
    "        for i in a:\n",
    "            price.append(i.text)\n",
    "\n",
    "    \n",
    "    while(len(price)<50):\n",
    "        extract_2(driver,50)\n",
    "        \n",
    "    while(len(price)<100):\n",
    "        next=driver.find_element_by_xpath(\"//li[@class='pagination-next']/a\")\n",
    "        driver_2=webdriver.Chrome(r\"C:\\\\chromedriver.exe\")\n",
    "        url=next.get_attribute('href')\n",
    "        driver_2.get(url)\n",
    "        extract_2(driver_2,50) \n",
    "    \n",
    "    jobs=pd.DataFrame({})\n",
    "    jobs['Brand']=brand\n",
    "    jobs['Description']=des\n",
    "    jobs['Price']=price\n",
    "    \n",
    "    driver.close()\n",
    "    driver_2.close()\n",
    "    \n",
    "    return(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=myntra('https://www.myntra.com/shoes')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
